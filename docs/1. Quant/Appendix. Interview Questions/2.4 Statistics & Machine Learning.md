# Statistics & Machine Learning
{% include "QuantInterviewQuestions/kmeans.md" %}


## Bias-Variance Tradeoff
A model can be thought of as a statistic (i.e., a function of samples), hence it has a sampling distribution with its mean and variance. The error between the mean (i.e., average model prediction) and the ground truth is called *bias*, while the average variability in the model prediction is called *variance*, which tells you how much the model changes as the data are sampled repeatedly. Mathematically,

\begin{align}
	\mbox{Bias} &= \EEE[\hat{f}(x)] - f(x)\\
	\mbox{Variance} &= \mbox{Var}[\hat{f}(x)]
\end{align}

The mean square error can then be decomposed into three terms:

\begin{align}	
	\mbox{Error} &\equiv \EEE[(\hat{f}(x) - f(x))^2] \\ \nonumber
	&= (\EEE[\hat{f}(x)-f(x)])^2 + \mbox{Var}[\hat{f}(x) - f(x)]\\ \nonumber
	&= (\EEE[\hat{f}(x)-f(x)])^2 + \mbox{Var}[\hat{f}(x)] + \mbox{Var}[f(x)] \\ \nonumber
	&= \mbox{Bias}^2 + \mbox{Variance} + \mbox{Ground Truth Error}
\end{align}

The immediate consequences of the above are that:

- Increased data size does not help the bias term.
- But it does help the variance term.
- The ground truth error, which comes from the population, is irreducible. One way to resolve this issue is to use mixture models and/or ensemble methods (read Section \ref{sec:Ensemble_Methods} for more details). See Questions (e.g., \ref{q:Bias-Variance Tradeoff}) for the topic of this section.
