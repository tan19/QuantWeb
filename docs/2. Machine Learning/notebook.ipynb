{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>0.385192</td>\n",
       "      <td>-1.572812</td>\n",
       "      <td>-0.427965</td>\n",
       "      <td>1.589609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>1.035777</td>\n",
       "      <td>-0.428113</td>\n",
       "      <td>0.210538</td>\n",
       "      <td>0.926154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>-0.540016</td>\n",
       "      <td>-0.458651</td>\n",
       "      <td>0.405547</td>\n",
       "      <td>1.109699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>0.141192</td>\n",
       "      <td>-0.904100</td>\n",
       "      <td>-2.229195</td>\n",
       "      <td>2.381928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-05</th>\n",
       "      <td>0.143568</td>\n",
       "      <td>0.762386</td>\n",
       "      <td>0.070902</td>\n",
       "      <td>1.329507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-06</th>\n",
       "      <td>0.417421</td>\n",
       "      <td>1.304776</td>\n",
       "      <td>1.054625</td>\n",
       "      <td>0.087306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   A         B         C         D\n",
       "2013-01-01  0.385192 -1.572812 -0.427965  1.589609\n",
       "2013-01-02  1.035777 -0.428113  0.210538  0.926154\n",
       "2013-01-03 -0.540016 -0.458651  0.405547  1.109699\n",
       "2013-01-04  0.141192 -0.904100 -2.229195  2.381928\n",
       "2013-01-05  0.143568  0.762386  0.070902  1.329507\n",
       "2013-01-06  0.417421  1.304776  1.054625  0.087306"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = pd.date_range('20130101', periods=6)\n",
    "df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"tip\">\n",
    "    <summary>Extra: Proof of decomposition</summary>\n",
    "    <p><br>First, let's recall conditional probability,<br>\n",
    "    $$P\\left (A|B\\right ) = \\frac{P\\left (A, B\\right )}{P\\left (B\\right )}$$\n",
    "    The above equation is so derived because of reduction of sample space of $A$ when $B$ has already occured.\n",
    "    Now, adjusting terms -<br>\n",
    "    $$P\\left (A, B\\right ) = P\\left (A|B\\right )*P\\left (B\\right )$$\n",
    "    This equation is called chain rule of probability. Let's generalize this rule for Bayesian Networks. The ordering of names of nodes is such that parent(s) of nodes lie above them (Breadth First Ordering).<br>\n",
    "    $$P\\left (X_1, X_2, X_3, ..., X_n\\right ) = P\\left (X_n, X_{n-1}, X_{n-2}, ..., X_1\\right )\\\\\n",
    "    = P\\left (X_n|X_{n-1}, X_{n-2}, X_{n-3}, ..., X_1\\right ) * P \\left (X_{n-1}, X_{n-2}, X_{n-3}, ..., X_1\\right ) \\left (Chain Rule\\right )\\\\  \n",
    "    = P\\left (X_n|X_{n-1}, X_{n-2}, X_{n-3}, ..., X_1\\right ) * P \\left (X_{n-1}|X_{n-2}, X_{n-3}, X_{n-4}, ..., X_1\\right ) * P \\left (X_{n-2}, X_{n-3}, X_{n-4}, ..., X_1\\right )$$\n",
    "    Applying chain rule repeatedly, we get the following equation -<br>\n",
    "    $$P\\left (\\bigcap_{i=1}^{n}X_i\\right ) = \\prod_{i=1}^{n} P\\left (X_i | P\\left (\\bigcap_{j=1}^{i-1}X_j\\right )\\right )$$\n",
    "    Keep the above equation in mind. Let's bring back Markov property. To bring some intuition behind Markov property, let's reuse <a href=\"#bayesian-network-example\">Bayesian Network Example</a>. If we say, the student scored very good  <strong>grades</strong>, then it is highly likely the student gets  <strong>acceptance letter </strong> to university. No matter how  <strong>difficult</strong> the class was, how much  <strong>intelligent </strong> the student was, and no matter what his/her  <strong>SAT</strong> score was. The key thing to note here is by  <strong>observing</strong> the node's parent, the influence by  <strong>non-descendants</strong> towards the node gets eliminated. Now, the equation becomes -<br>\n",
    "    $$P\\left (\\bigcap_{i=1}^{n}X_i\\right ) = \\prod_{i=1}^{n} P\\left (X_i | Par\\left (X_i\\right )\\right )$$\n",
    "    Bingo, with the above equation, we have proved  <strong>Factorization Theorem </strong> in Probability.\n",
    "    </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mkdocs-material things\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "    <p class=\"admonition-title\">Note</p>\n",
    "    <p>\n",
    "        If two distributions are similar, then their entropies are similar, implies the KL divergence with respect to two distributions will be smaller. And vica versa. In Variational Inference, the whole idea is to <strong>minimize</strong> KL divergence so that our approximating distribution $q(\\theta)$ can be made similar to $p(\\theta|D)$.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"note\">\n",
    "<summary>Empty Class in C++</summary>\n",
    "<div class=\"tabbed-set tabbed-alternate\" data-tabs=\"1:3\"><input checked=\"checked\" id=\"__tabbed_1_1\" name=\"__tabbed_1\" type=\"radio\" /><input id=\"__tabbed_1_2\" name=\"__tabbed_1\" type=\"radio\" /><input id=\"__tabbed_1_3\" name=\"__tabbed_1\" type=\"radio\" /><div class=\"tabbed-labels\"><label for=\"__tabbed_1_1\">Question</label><label for=\"__tabbed_1_2\">Answer</label><label for=\"__tabbed_1_3\">Comment</label></div>\n",
    "<div class=\"tabbed-content\">\n",
    "<div class=\"tabbed-block\">\n",
    "<p>What does the compiler create for an empty class in C++?</p>\n",
    "</div>\n",
    "<div class=\"tabbed-block\">\n",
    "<p>A class with no virtual functions occupies 1 byte; otherwise, it occupies 4 or 8 bytes, depending on the machine is 32-bit or 64-bit.</p>\n",
    "<p>For any object of an empty class, 1 byte is allocated by compiler for unique address identification. The minimum amount of memory is 1 byte.</p>\n",
    "<p>For a class without any data members but with at lest one virtual function, its object occupies 4 bytes (on 32-bit machines) or 8 bytes (on 64-bit machines), because a hidden virtual point (called VPTR) is created as a class (all objects share one) data member field. Hence an empty class with virtual functions is actually no longer empty.</p>\n",
    "</div>\n",
    "<div class=\"tabbed-block\"></div>\n",
    "</div>\n",
    "</div>\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7739006a151c9665ad246e9073187ed95c6d3191911b84a76a57a0f8373bf5d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
