# Basics of Machine Learning


~
== Top Journal in Machine Learning
- Journal of Machine Learning Research (JMLR)
- Conference On Learning Theory (COLT)
- Machine Learning Journal (MLJ)

== Top Conferences in Machine Learning
- Neural Information processing systems (NIPS)
- International conference on machine learning (ICML)
- UAI
- AISTATS
- Others: KDD, AAAI, ACL, CVPR, ICLR
~

~
{}{img_left}{ML_FiveTribes.png}{}{800}{800}{}
~

[http://mlss.cc/ Machine Learning Summer School]

[https://github.com/kailashahirwar/cheatsheets-ai Machine Learning Cheat Sheet]


- [http://ciml.info/ A Course in Machine Learning]
- [http://khashab2.web.engr.illinois.edu/learn.html Learning Algorithms]


                       |-- Introduction            : [https://work.caltech.edu/telecourse.html Caltech], [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/ AI course]
                       |
                       |-- Linear Programming      : [https://class.coursera.org/linearprogramming-001 Coursera]                       
                       |
                       |-- VB                      : [http://www.variational-bayes.org Repository]
                       |-- EP                      : [http://research.microsoft.com/en-us/um/people/minka/papers/ep/roadmap.html Roadmap]                       
                       |-- MCMC                    : [http://onionesquereality.wordpress.com/2008/08/31/demystifying-support-vector-machines-for-beginners/ Blog], [http://www.statslab.cam.ac.uk/~mcmc/ Repository], [http://www.cs.toronto.edu/~radford/ftp/review.pdf Neal], [http://www.people.fas.harvard.edu/~junliu/TechRept/99folder/mcmc.pdf Liu (1)], [http://www.amazon.com/Monte-Carlo-Strategies-Scientific-Computing/dp/0387952306 Liu (2)], [http://www.csss.washington.edu/Papers/wp9.pdf Besag], [http://www.amazon.com/Markov-Chain-Monte-Carlo-Statistical/dp/1584885874 Gamerman & Lopes], [http://homepages.inf.ed.ac.uk/imurray2/pub/07thesis/murray_thesis_2007.pdf Murray], [https://www.youtube.com/watch?v=RJjcBwRiR38&index=19&list=PLFHD4aOUZFp3Fx3rfRkBR0XjP1OCcrYXP M-G]
                       |
                       |-- NN                      : [https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH Hugo Larochelle's tutorial], [https://class.coursera.org/neuralnets-2012-001 Geoffrey Hinton's course], [ftp://ftp.sas.com/pub/neural/FAQ.html FAQ], [http://www.inference.phy.cam.ac.uk/mackay/Bayes_FAQ.html Bayesian NN FAQ], [http://karpathy.github.io/neuralnets/
 NN Guide], [http://neuralnetworksanddeeplearning.com/index.html NN Guide2]
                       |-- PGM                     : [http://www.bayesnets.com/ BNs Repository]
 Machine Learning  |-- DL                      : [http://www.iro.umontreal.ca/~bengioy/dlbook/ book]
                       |
                       |-- GP                      : [Code/GP_example.m code], [http://mlg.eng.cam.ac.uk/duvenaud/cookbook/index.html covariance function], [http://www.carlboettiger.info/2012/10/17/basic-regression-in-gaussian-processes GP regression tutorial], [http://www.inference.eng.cam.ac.uk/mng10/GP/ resources]                                          
                       |-- CRP                     : [Code/CRP.m code]                       
                       |
                       |-- SVM                     : [http://www.svms.org/ Repository], [http://research.microsoft.com/pubs/67119/svmtutorial.pdf Burges' SVM tutorial]
                       |-- Kernel Methods          : [http://www.kernel-machines.org/ Kernel], [http://alex.smola.org/papers/2003/SchSmo03c.pdf Introduction]                       
                       |-- Deep Learning           : [http://deeplearning.net/ Repository], [http://neuralnetworksanddeeplearning.com/ Tutorial1], [http://deeplearning.net/tutorial/ Tutorial2], [http://ufldl.stanford.edu/tutorial/ Tutorial3], [http://karpathy.github.io/neuralnets/ Blog1], [http://karpathy.github.io/2015/05/21/rnn-effectiveness/ Blog2], [http://karpathy.github.io/2015/10/25/selfie/ Blog3], [http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf Paper1], [http://arxiv.org/pdf/1206.5538v3.pdf Paper2], [http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf Paper3], [http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf Paper4], [http://deeplearning.net/ Website], [http://www.deeplearningbook.org/ Book]                    


- [http://homepages.inf.ed.ac.uk/rbf/IAPR/researchers/MLPAGES/mlbks.htm Resources], [http://scikit-learn.org/ scikit], [https://probmods.org/ Church]
- A blog post on CRP, DP, and more: [http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/ Link]
- Research Blogs: [http://www.37steps.com 37steps]
- AMA: [http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun Yann Lecun], [http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio Yoshua Bengio], [https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/ Michael Jordan], [https://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/ Andrew Ng]
- Tutorials: [http://www.autonlab.org/tutorials/index.html Tutorial Slides by Andrew Moore]



== The Essentials
- Pattern Recognition and Machine Learning [http://www.springer.com/cda/content/document/cda_downloaddocument/9780387310732-t1.pdf toc], [https://github.com/PRML/PRMLT Code Repository]   
- Machine Learning: A Probabilistic Perspective [https://www.cs.ubc.ca/~murphyk/MLbook/pml-toc-22may12.pdf toc]   
- Foundations of Machine Learning [https://mitpress.mit.edu/sites/default/files/titles/content/9780262018258_toc_0001.pdf toc]
- Convex Optimization [http://assets.cambridge.org/97805218/33783/toc/9780521833783_toc.pdf toc], [https://class.stanford.edu/courses/Engineering/CVX101/Winter2014/info courseware]
- [http://users.iems.northwestern.edu/~nocedal/book/toc.html Numerical Optimization]
- Machine Learning: A Bayesian and Optimization Perspective
- Pattern Recognition
- The Elements of Statistical Learning [http://statweb.stanford.edu/~tibs/ElemStatLearn/contents.pdf toc], [http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf Ebook]
- Bayesian Reasoning and Machine Learning [http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/181115.pdf Ebook]   
- Introduction to Linear Optimization
- Nonlinear Programming

== Others
- MCMC
-- Radford M. Neal's work of MCMC on DP ([http://www.cs.toronto.edu/~radford/ftp/bmm.pdf 1991], [http://www.cs.toronto.edu/~radford/ftp/mixmc.pdf 1998], [http://www.cs.toronto.edu/~radford/ftp/mixsplit.pdf 2000], [http://www.cs.toronto.edu/~radford/ftp/mixsplit2.pdf 2005]).
-- [http://www.people.fas.harvard.edu/~junliu/TechRept/94folder/collaps2.pdf Collapsed Gibbs Sampler, by Jun S. Liu]

- Variational Methods
-- [http://www.cs.ubc.ca/~murphyk/Papers/EP.ps.gz From BP to EP, by Keven P Murphy]

- Causal Inference
-- [http://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf Causal inference in statistics: An overview]

- Kernel Methods
-- [http://www.umiacs.umd.edu/~hal/docs/daume04rkhs.pdf RKHS tutorial, by Hal Daum`e III]


- Top Conferences and Journals: [http://jmlr.csail.mit.edu/proceedings JMLR] [http://books.nips.cc NIPS] [http://www.machinelearning.org/icml.html ICML] [http://uai.sis.pitt.edu/proceedings.jsp?mmnu=1&smnu=0 UAI]
- Repository: [http://machinelearning.wustl.edu/mlpapers/venues Repository]   

## Supervised Learning and Unsupervised Learning
The two major tasks in Machine Learning are *supervised learning* and *unsupervised learning*. In supervised learning, where you have input variables $x_i$ and the tags $t_i$, your tasks are to model (as an approximation) the relationship $f$ between them, i.e., 
\(
\hat t_i = y_i = f(x_i)
\)
These tasks are called classification when $t_i$ and $y_i$ are discrete, and regression when continuous. In unsupervised learning, however, you only have input variables $x_i$ and tags $t_i$ are not available to you. The tasks are usually:
- *clustering*, where your $y_i$ are discrete (a.k.a "clusters");
- *density estimation*, where your $y_i$ are often continuous (more familiar if we write $p(x_i)$ instead);
- *feature learning*, where we only work on $x_i$ themselves and try to discover better representations of them.

=== Frequentist v.s. Bayesian
- From a *Frequentist* perspective, parameters $\bf{w}$ are considered to be fixed but unknown constants, whose values are estimated by some estimators, and error bars around the realized estimate are obtained by considering the distribution of possible data sets (samples), i.e., the estimator itself has a sampling distribution. *Bootstrapping* (re-sample the observed sample with replacement) is a popular method to determining such error bars.
- From a *Bayesian* perspective, there is only one single data set (namely the one that is actually observed), and the parameters $\bf{w}$ are considered to be random variables, the uncertainty of which are expressed through probability distributions over $\bf{w}$.

Bayesian models enjoy the benefits of flexibility and expressiveness, but at the same time suffer from computational bottlenecks. However, thanks to the recent development of computing power, Monte Carlo methods are made possible for a wide range of applications. In addition, the creation of a set of highly efficient deterministic approximation algorithms such as VB and EP, offers an alternative to the sampling methods.

=== Generative Models v.s. Discriminative Models
- Generative models model $p(x,t)$. For example: Naive Bayes.
- Discriminative models model $p(t|x)$. For example: Logistic Regression.

Generative models involve more computations but with the flexibility to, for example, learn $p(x)$, which can be used for outlier detection. Discriminative models often have less application but are computationally more efficient than generative models.

=== Over-fitting
*Over-fitting* means the model is tuned to the random noise on the target values $t_i$, and hence would have sub-optimal performance on test data sets. There are two ways to address this issue:
- In the Frequentist framework, regularization is often used, e.g., regression with L-1 regularization (a.k.a. ``Lasso Regression''), and with L-2 regularization (a.k.a. ``Ridge Regression''). The regularization parameter is often tuned with a *validation data set*, or using the *cross-validation* technique if data are not abundant.
- In the Bayesian framework, the effective number of parameters adapts automatically (/at the cost of more computations/) to the size of the data set. In some cases, it is equivalent to the Frequentist method.

=== Curse of Dimensionality
The curse of dimensionality implies that we need significantly large quantity of data points to fit our model. This is because that, for a given number of data points, the higher the dimension, the lower the probability that they would reside close together, which breaks the homogeneity assumption of statistical models.

=== Information Theory
- The maximum entropy distribution in a closed interval is uniform.
- The maximum entropy distribution given its mean and variance is Gaussian.
- KL divergence is asymmetric.
- Entropy $H(p)$ is concave in $p$.
- Mutual information $I(x,y)$ is concave of $p(x)$ given $p(y|x)$ and convex of $p(y|x)$ given $p(x)$.