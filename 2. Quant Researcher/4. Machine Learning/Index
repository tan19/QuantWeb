# Basics of Machine Learning

\begin{question}{Hyper-parameter Tuning}{$\star$}{Machine Learning; Hyper-parameter}
What are the common hyper-parameter tuning methods?
\end{question}

\section{Bias-Variance Tradeoff}
A model can be thought of as a statistic (i.e., a function of samples), hence it has a sampling distribution with its mean and variance. The error between the mean (i.e., average model prediction) and the ground truth is called \emph{bias}, while the average variability in the model prediction is called \emph{variance}, which tells you how much the model changes as the data are sampled repeatedly. Mathematically,
\begin{align*}
	\mbox{Bias} &= \EEE[\hat{f}(x)] - f(x)\\
	\mbox{Variance} &= \mbox{Var}[\hat{f}(x)]
\end{align*}
The mean square error can then be decomposed into three terms:
\begin{align}	
	\mbox{Error} &\equiv \EEE[(\hat{f}(x) - f(x))^2] \\ \nonumber
	&= (\EEE[\hat{f}(x)-f(x)])^2 + \mbox{Var}[\hat{f}(x) - f(x)]\\ \nonumber
	&= (\EEE[\hat{f}(x)-f(x)])^2 + \mbox{Var}[\hat{f}(x)] + \mbox{Var}[f(x)] \\ \nonumber
	&= \mbox{Bias}^2 + \mbox{Variance} + \mbox{Ground Truth Error}
\end{align}
The immediate consequences of the above are that 1) increased data size does not help the bias term, 2) but it does help the variance term, and 3) the ground truth error, which comes from the population, is irreducible. One way to resolve this issue is to use mixture models and/or ensemble methods (read Section \ref{sec:Ensemble_Methods} for more details). See Questions (e.g., \ref{q:Bias-Variance Tradeoff}) for the topic of this section.

\begin{figure}
\centering
\includegraphics[scale=0.5]{Bias-Variance-Tradeoff.jpg}
\caption{Bias-Variance Tradeoff.}
\end{figure}

\section{Bayesian vs Frequentist}
Suppose \emph{under the null hypothesis} (which may or may not be true), the sample random variable $\X$ follows the distribution $P(\X|\bTheta)$, which is parameterized by $\bTheta$. To learn about $\bTheta$ under this null hypothesis, we draw a fix-sized sample of i.i.d. observations from the population, large enough to be representative:
\begin{align*}
\x_0 \equiv \X(\omega_0) = \{X_1(\omega_0), \cdots, X_n(\omega_0)\}
\end{align*}
This particular observed sample $\x$, which is of course random in nature, however draws two fundamentally different schools of interpretations.

\subsection{Frequentist View}
The \emph{Frequentist view} is that:

\vspace{0.5cm}
\noindent\fbox{%
    \parbox{\textwidth}{%
        The randomness of the sample $\X$ only comes from the sampling procedure, not the fixed but unknown parameter $\bTheta$.
    }%
}
\vspace{0.3cm}

\noindent
Hence it makes sense to state $P(0 \le \hat{\theta} \le 1) = 0.5$ but not the case for $P(0 \le \theta \le 1) = 0.5$, because $\hat{\theta}$ is a random variable which has a sampling distribution whereas $\theta$ is a fixed but unknown constant, i.e., it is either $P(0 \le \theta \le 1) = 1$ or $P(0 \le \theta \le 1) = 0$.

To learn the \emph{unknown constant} parameter $\bTheta$, Frequentists propose to maximize the likelihood function (MLE), which is the parameter perspective of the observations' joint distribution (usually not a valid probability distribution, hence the name ``function''):
\begin{align*}
	\hat{\bTheta}_{MLE} = \mbox{argmax}_\bTheta ~ L(\bTheta; \X = \x_0) = \mbox{argmax}_\bTheta ~ P(\X = \x_0|\bTheta)
\end{align*}

\subsection{Bayesian View}
The Bayesian view, however, is hierarchical in nature:

\vspace{0.5cm}
\noindent\fbox{%
    \parbox{\textwidth}{%
        The randomness of the sample $\X$ comes from the local distributions \emph{and} their local parameters, which themselves are random variables.
    }%
}
\vspace{0.3cm}

Just imagine that we want to measure the average height of all the population in the world. Frequentists have a \emph{grand population distribution} while Bayesians have a collection of \emph{local distributions} with their own parameters for each and every ``household'', which may or may not come from the same family of distributions let alone the Frequentist grand population distribution. Usually, however, for computational and modeling convenience, these local distributions are assumed to be within the same distribution family, e.g., exponential family, and their common and shared parameters are governed by the corresponding prior distributions.

To learn the \emph{random variable} parameter $\bTheta$, Bayesians propose to maximize the aposteriori probability (MAP), which is the posterior distribution of the parameter given the observed data:
\begin{align*}
	\hat{\bTheta}_{MAP} &= \mbox{argmax}_\bTheta ~ P(\bTheta | \X = \x_0) \\
	&= \mbox{argmax}_\bTheta ~ \frac{P(\X = \x_0 | \bTheta) P(\bTheta)}{P(\X = \x_0)}\\
	&= \mbox{argmax}_\bTheta ~ P(\X = \x_0|\bTheta) P(\bTheta)
\end{align*}
where $P(\X = \x_0)$ is called the ``evidence'', $P(\bTheta)$ the ``prior'', and $P(\X = \x_0 | \bTheta)$ the ``likelihood''. See Questions (e.g., \ref{q:Bayesian vs Frequentist}) for the topic of this section.


\section{Exponential Family}
\begin{align}
	f(y|\theta,\phi) & = \exp \left\{ \frac{y\theta-b(\theta)}{a(\phi)}+c(y,\phi) \right\} \\
	E(y) & = b'(\theta) \\
	Var(y) & = b''(\theta)a(\phi)
\end{align}

\section{Overview}
There are two main types of Machine Learning problems: the {\bf{predictive}} or {\bf{supervised learning}}, and the {\bf{descriptive}} or {\bf{unsupervised learning}} \footnote{It is also called {\bf{Knowledge Discovery}} in the Data Mining literature}.

For supervised learning, there are two subtypes: the {\bf{classification}} problem, where the response variable is categorical (either ordinal or nomial); and the {\bf{regression}} problem, where the response variable is numerical (either discrete or continuous).

For unsupervised learning, it is also called {\bf{density estimation}} in Statistics literature. Essentially we want to build models of the form $p(\x_i|\theta)$, and supervised learning can be seen as to build models of the form $p(y_i|\x_i,\theta)$, which is a problem of conditional density estimation. \footnote{We see from the formulation that, unsupervised learning usually involves multivariate probability models while supervised learning usually involves univariate probability models.}


\subsection{Some Distinctions}
\subsection{Machine Learning v.s. Statistical Learning}
{\bf{Machine Learning}} focuses more on high dimension low noise situations, and performance and efficiency are the main concerns. {\bf{Statistical Learning}} focuses more on low dimension high noise situations, interpretation and statistical inference are the main concerns.
\subsubsection{Parametric v.s. Non-parametric Models}
A parametric model has a fixed {\em{finite}} number of parameters, while the number of parameters in a non-parametric model grows with the amount of training data. A model with infinite number of parameters is usually non-parametric.

\subsection{A Brief History of Machine Learning}
The perceptron model was invented in 1957, and it generated over optimistic view for AI during 1960s. After Marvin Minsky pointed out the limitation of this model in expressing complex functions, researchers stopped pursuing this model for the next decade.

In 1970s, the machine learning field was dormant, when expert systems became the mainstream approach in AI.  The revival of machine learning came in mid-1980s, when the decision tree model was invented and distributed as software. It is also in mid 1980s multi-layer neural networks were invented, With enough hidden layers, a neural network can express any function, thus overcoming the limitation of perceptron. We see a revival of the neural network study.

Around 1995, SVM was proposed and have become quickly adopted.

After year 2000, Logistic regression was rediscovered and re-designed for large scale machine learning problems . In the ten years following 2003, logistic regression has attracted a lot of research work.

We discussed the development of 4 major machine learning methods. There are other method developed in parallel, but see declining use today in the machine field: Naive Bayes, Bayesian networks, and Maximum Entropy classifier (most used in natural language processing).

In addition to the individual methods, we have seen the invention of ensemble learning, where several classifiers are used together, and its wide adoption today.


\url{http://en.wikipedia.org/wiki/Timeline_of_artificial_intelligence}

1950s-1960s: the Perceptron Model
1970s-1980s: the Expert Systems
mid 1980s: Multi-layer Neural Networks
1995: SVM
2000s: Logistic Regression Rediscovered



\subsection{Regression v.s. Classification}
Consider a supervised learning problem in which we wish to approximate an unknown target function $f: \XX \to \YY$, or equivalently $P(Y|X)$. One way to learn $P(Y|X)$ is to use the training data to estimate $P(X|Y)$ and $P(Y)$, and then use Bayes rule to determine $P(Y|X)$.
\subsection{Parametric v.s. Nonparametric Models}


\begin{table}[]
\centering
\begin{tabular}{| l | l | l | l | l |}
\hline
\bf{Model} & \bf{Supervised} & \bf{Generative} & \bf{Advantages} & \bf{Disadvantages} \\\hline
Naive Bayes & Yes & ? & & \\\hline
\end{tabular}
\caption{Model comparison.}
\end{table}


\input{Ensemble}

\section{Statistical Learning Theory}
Statistical learning theory studies the properties, in particular error-bounds, of learning algorithms in a statistical framework.

The No Free Lunch theorem says, if no assumptions about how training data are related to the testing data, prediction is impossible; furthermore, if no assumptions about the data to be expected, generalization is impossible.

Simply means we need to make the assumption that there is a stationary distribution of data.

\deff{
	Suppose $f: \RRR \to \RRR_+$ and $g: \RRR \to \RRR_+$, we write
	\begin{align}
		f = O(g)		
	\end{align}	
	if there exists $x_0, \alpha \in \RRR_+$ such that for all $x > x_0$ we have $f(x) \le \alpha g(x)$. Replacing $\le$ with $\ge$, we write $f = \Omega(g)$.
}

\deff{
	Suppose $f: \RRR \to \RRR_+$ and $g: \RRR \to \RRR_+$, we write
	\begin{align}
		f = o(g)
	\end{align}	
	if for every $\alpha > 0$ there exists $x_0$ such that for all $x > x_0$ we have $f(x) \le \alpha g(x)$. Replacing $\le$ with $\ge$, we write $f = \omega(g)$.
}

\deff{
	If $f = O(g)$ and $f = \Omega(g)$, then we write $f = \Theta(g)$.
}

\deff{
	We write $f = \tilde O(g)$ if there exists $k \in \NNN$ such that $f(x) = O(g(x)\log^k(g(x)))$.
}

\section{Parametric and nonparametric models}
In a set of probability spaces $\{(\YY, \FF, \PP_\Theta)\}$, a {\em{statistical model}} $\MM$ on a sample space $\YY$ is {\underline{a set of probability measures}} $\PP_\Theta$ on $\YY$. If we write $PM(\YY)$ for the space of all probability measure on $\YY$, a model is a subset $\MM \subset PM(\YY)$. Every element of $\MM$ has a one-to-one mapping (hence the model is {\em{identifiable}}) with its parameter $\btheta$ with values in a parameter space $\Theta$, that is,
\begin{align}
\MM(\y) = \{P_\btheta(\y) | \btheta \in \Theta\},\quad \y \in \YY.
\end{align}
For example, a first order polynomial is a model, and a second order polynomial is another model. We can of course fit a model to the observed data, but {\em{model}} itself is an abstract concept, where the parameter values of a model need not be specified. We call a model {\em{parametric}} if $\Theta$ has finite dimension, and {\em{nonparametric}} if $\Theta$ has infinite dimension.

To formulate statistical problems, we assume that $n$ observations $\y_1,\dots,\y_n$ with values in $\YY$ are observed, which are drawn i.i.d. from a measure $P_\btheta$ in the model, i.e.,
\begin{align}
\y_1, \dots, \y_n \sim_{iid} P_\btheta \qquad \text{for some}~~ \btheta \in \Theta
\end{align}
The objective of statistical {\em{inference}} is then to draw conclusions about the value of $\btheta$ (and hence about the distribution $P_\btheta$ of the data) from the observations.


\section{Bayesian and Bayesian nonparametric models}
In Bayesian statistics, all parameters are considered as random variables. Hence under a Bayesian model, data are generated in two stages, i.e.,
\begin{align}
\btheta &\sim P(\btheta)\\
\y_1, \dots, \y_n ~|~ \btheta &\sim_{iid} P_\btheta(\y)
\end{align}
The objective is then to determine the {\em{posterior distribution}} -- the conditional distribution of $\btheta$ given the observed data,
\begin{align}
\pi(\btheta | \y_1, \dots, \y_n)
\end{align}
A {\em{Bayesian nonparametric}} model is a Bayesian model whose parameter space $\Theta$ has infinite dimension. To define a Bayesian nonparametric model, we have to define a prior $\pi$ on an infinite-dimensional space, which is a stochastic process with paths (i.e. realizations) in $\Theta$.


\section{Bayesian Nonparametrics}
The concept of functions can be generalized to that of algorithms, which describe procedures, with loops and conditional tests, of how to generate output from input.

A draw from a finite dimensional Gaussian distribution is a real number, while a real-valued function can be considered as a sequence of (uncountably) infinite number of real numbers. A Gaussian Process (GP) is an infinite dimensional generalization of a Gaussian distribution. It defines a prior over real-valued functions, and a sample of it is a particular example of such functions.

A draw from a finite dimensional Dirichlet distribution is a (discrete) probability measure. A Dirichlet Process (DP) is an infinite dimensional generalization of a Dirichlet distribution. It defines a prior over probability measures, and a sample of it is a probability measure.  Distributions drawn from a Dirichlet process are discrete, but cannot be described using a finite number of parameters, thus the classification as a nonparametric model.


Note, that we do not have a measurement of the function, as in the GP case but a sample of the true probability measure; this is the main difference between GP and DP.

This note is based on Peter Orbanz's BNP notes:
\vspace*{5mm}
\\
\url{http://stat.columbia.edu/~porbanz/npb-tutorial.html}

\subsection{Notation}
Bold upper case letters represent matrices, e.g., $\X, \Y, \Z, \bTheta$. Bold lower case letters represent vector-valued random variables and their realizations (we do not distinguish between the two), e.g., $\x, \y, \z, \btheta$. Curly upper case letters represent spaces (i.e., possible values) of random variables, e.g., $\XX, \YY, \ZZ, \Theta$.

~
== Top Journal in Machine Learning
- Journal of Machine Learning Research (JMLR)
- Conference On Learning Theory (COLT)
- Machine Learning Journal (MLJ)

== Top Conferences in Machine Learning
- Neural Information processing systems (NIPS)
- International conference on machine learning (ICML)
- UAI
- AISTATS
- Others: KDD, AAAI, ACL, CVPR, ICLR
~

~
{}{img_left}{ML_FiveTribes.png}{}{800}{800}{}
~

[http://mlss.cc/ Machine Learning Summer School]

[https://github.com/kailashahirwar/cheatsheets-ai Machine Learning Cheat Sheet]


- [http://ciml.info/ A Course in Machine Learning]
- [http://khashab2.web.engr.illinois.edu/learn.html Learning Algorithms]


                       |-- Introduction            : [https://work.caltech.edu/telecourse.html Caltech], [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/ AI course]
                       |
                       |-- Linear Programming      : [https://class.coursera.org/linearprogramming-001 Coursera]                       
                       |
                       |-- VB                      : [http://www.variational-bayes.org Repository]
                       |-- EP                      : [http://research.microsoft.com/en-us/um/people/minka/papers/ep/roadmap.html Roadmap]                       
                       |-- MCMC                    : [http://onionesquereality.wordpress.com/2008/08/31/demystifying-support-vector-machines-for-beginners/ Blog], [http://www.statslab.cam.ac.uk/~mcmc/ Repository], [http://www.cs.toronto.edu/~radford/ftp/review.pdf Neal], [http://www.people.fas.harvard.edu/~junliu/TechRept/99folder/mcmc.pdf Liu (1)], [http://www.amazon.com/Monte-Carlo-Strategies-Scientific-Computing/dp/0387952306 Liu (2)], [http://www.csss.washington.edu/Papers/wp9.pdf Besag], [http://www.amazon.com/Markov-Chain-Monte-Carlo-Statistical/dp/1584885874 Gamerman & Lopes], [http://homepages.inf.ed.ac.uk/imurray2/pub/07thesis/murray_thesis_2007.pdf Murray], [https://www.youtube.com/watch?v=RJjcBwRiR38&index=19&list=PLFHD4aOUZFp3Fx3rfRkBR0XjP1OCcrYXP M-G]
                       |
                       |-- NN                      : [https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH Hugo Larochelle's tutorial], [https://class.coursera.org/neuralnets-2012-001 Geoffrey Hinton's course], [ftp://ftp.sas.com/pub/neural/FAQ.html FAQ], [http://www.inference.phy.cam.ac.uk/mackay/Bayes_FAQ.html Bayesian NN FAQ], [http://karpathy.github.io/neuralnets/
 NN Guide], [http://neuralnetworksanddeeplearning.com/index.html NN Guide2]
                       |-- PGM                     : [http://www.bayesnets.com/ BNs Repository]
 Machine Learning  |-- DL                      : [http://www.iro.umontreal.ca/~bengioy/dlbook/ book]
                       |
                       |-- GP                      : [Code/GP_example.m code], [http://mlg.eng.cam.ac.uk/duvenaud/cookbook/index.html covariance function], [http://www.carlboettiger.info/2012/10/17/basic-regression-in-gaussian-processes GP regression tutorial], [http://www.inference.eng.cam.ac.uk/mng10/GP/ resources]                                          
                       |-- CRP                     : [Code/CRP.m code]                       
                       |
                       |-- SVM                     : [http://www.svms.org/ Repository], [http://research.microsoft.com/pubs/67119/svmtutorial.pdf Burges' SVM tutorial]
                       |-- Kernel Methods          : [http://www.kernel-machines.org/ Kernel], [http://alex.smola.org/papers/2003/SchSmo03c.pdf Introduction]                       
                       |-- Deep Learning           : [http://deeplearning.net/ Repository], [http://neuralnetworksanddeeplearning.com/ Tutorial1], [http://deeplearning.net/tutorial/ Tutorial2], [http://ufldl.stanford.edu/tutorial/ Tutorial3], [http://karpathy.github.io/neuralnets/ Blog1], [http://karpathy.github.io/2015/05/21/rnn-effectiveness/ Blog2], [http://karpathy.github.io/2015/10/25/selfie/ Blog3], [http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf Paper1], [http://arxiv.org/pdf/1206.5538v3.pdf Paper2], [http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf Paper3], [http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf Paper4], [http://deeplearning.net/ Website], [http://www.deeplearningbook.org/ Book]                    


- [http://homepages.inf.ed.ac.uk/rbf/IAPR/researchers/MLPAGES/mlbks.htm Resources], [http://scikit-learn.org/ scikit], [https://probmods.org/ Church]
- A blog post on CRP, DP, and more: [http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/ Link]
- Research Blogs: [http://www.37steps.com 37steps]
- AMA: [http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun Yann Lecun], [http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio Yoshua Bengio], [https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/ Michael Jordan], [https://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/ Andrew Ng]
- Tutorials: [http://www.autonlab.org/tutorials/index.html Tutorial Slides by Andrew Moore]



== The Essentials
- Pattern Recognition and Machine Learning [http://www.springer.com/cda/content/document/cda_downloaddocument/9780387310732-t1.pdf toc], [https://github.com/PRML/PRMLT Code Repository]   
- Machine Learning: A Probabilistic Perspective [https://www.cs.ubc.ca/~murphyk/MLbook/pml-toc-22may12.pdf toc]   
- Foundations of Machine Learning [https://mitpress.mit.edu/sites/default/files/titles/content/9780262018258_toc_0001.pdf toc]
- Convex Optimization [http://assets.cambridge.org/97805218/33783/toc/9780521833783_toc.pdf toc], [https://class.stanford.edu/courses/Engineering/CVX101/Winter2014/info courseware]
- [http://users.iems.northwestern.edu/~nocedal/book/toc.html Numerical Optimization]
- Machine Learning: A Bayesian and Optimization Perspective
- Pattern Recognition
- The Elements of Statistical Learning [http://statweb.stanford.edu/~tibs/ElemStatLearn/contents.pdf toc], [http://statweb.stanford.edu/~tibs/ElemStatLearn/printings/ESLII_print10.pdf Ebook]
- Bayesian Reasoning and Machine Learning [http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/181115.pdf Ebook]   
- Introduction to Linear Optimization
- Nonlinear Programming

== Others
- MCMC
-- Radford M. Neal's work of MCMC on DP ([http://www.cs.toronto.edu/~radford/ftp/bmm.pdf 1991], [http://www.cs.toronto.edu/~radford/ftp/mixmc.pdf 1998], [http://www.cs.toronto.edu/~radford/ftp/mixsplit.pdf 2000], [http://www.cs.toronto.edu/~radford/ftp/mixsplit2.pdf 2005]).
-- [http://www.people.fas.harvard.edu/~junliu/TechRept/94folder/collaps2.pdf Collapsed Gibbs Sampler, by Jun S. Liu]

- Variational Methods
-- [http://www.cs.ubc.ca/~murphyk/Papers/EP.ps.gz From BP to EP, by Keven P Murphy]

- Causal Inference
-- [http://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf Causal inference in statistics: An overview]

- Kernel Methods
-- [http://www.umiacs.umd.edu/~hal/docs/daume04rkhs.pdf RKHS tutorial, by Hal Daum`e III]


- Top Conferences and Journals: [http://jmlr.csail.mit.edu/proceedings JMLR] [http://books.nips.cc NIPS] [http://www.machinelearning.org/icml.html ICML] [http://uai.sis.pitt.edu/proceedings.jsp?mmnu=1&smnu=0 UAI]
- Repository: [http://machinelearning.wustl.edu/mlpapers/venues Repository]   

## Supervised Learning and Unsupervised Learning
The two major tasks in Machine Learning are *supervised learning* and *unsupervised learning*. In supervised learning, where you have input variables $x_i$ and the tags $t_i$, your tasks are to model (as an approximation) the relationship $f$ between them, i.e., 
\(
\hat t_i = y_i = f(x_i)
\)
These tasks are called classification when $t_i$ and $y_i$ are discrete, and regression when continuous. In unsupervised learning, however, you only have input variables $x_i$ and tags $t_i$ are not available to you. The tasks are usually:
- *clustering*, where your $y_i$ are discrete (a.k.a "clusters");
- *density estimation*, where your $y_i$ are often continuous (more familiar if we write $p(x_i)$ instead);
- *feature learning*, where we only work on $x_i$ themselves and try to discover better representations of them.

=== Frequentist v.s. Bayesian
- From a *Frequentist* perspective, parameters $\bf{w}$ are considered to be fixed but unknown constants, whose values are estimated by some estimators, and error bars around the realized estimate are obtained by considering the distribution of possible data sets (samples), i.e., the estimator itself has a sampling distribution. *Bootstrapping* (re-sample the observed sample with replacement) is a popular method to determining such error bars.
- From a *Bayesian* perspective, there is only one single data set (namely the one that is actually observed), and the parameters $\bf{w}$ are considered to be random variables, the uncertainty of which are expressed through probability distributions over $\bf{w}$.

Bayesian models enjoy the benefits of flexibility and expressiveness, but at the same time suffer from computational bottlenecks. However, thanks to the recent development of computing power, Monte Carlo methods are made possible for a wide range of applications. In addition, the creation of a set of highly efficient deterministic approximation algorithms such as VB and EP, offers an alternative to the sampling methods.

=== Generative Models v.s. Discriminative Models
- Generative models model $p(x,t)$. For example: Naive Bayes.
- Discriminative models model $p(t|x)$. For example: Logistic Regression.

Generative models involve more computations but with the flexibility to, for example, learn $p(x)$, which can be used for outlier detection. Discriminative models often have less application but are computationally more efficient than generative models.

=== Over-fitting
*Over-fitting* means the model is tuned to the random noise on the target values $t_i$, and hence would have sub-optimal performance on test data sets. There are two ways to address this issue:
- In the Frequentist framework, regularization is often used, e.g., regression with L-1 regularization (a.k.a. ``Lasso Regression''), and with L-2 regularization (a.k.a. ``Ridge Regression''). The regularization parameter is often tuned with a *validation data set*, or using the *cross-validation* technique if data are not abundant.
- In the Bayesian framework, the effective number of parameters adapts automatically (/at the cost of more computations/) to the size of the data set. In some cases, it is equivalent to the Frequentist method.

=== Curse of Dimensionality
The curse of dimensionality implies that we need significantly large quantity of data points to fit our model. This is because that, for a given number of data points, the higher the dimension, the lower the probability that they would reside close together, which breaks the homogeneity assumption of statistical models.

=== Information Theory
- The maximum entropy distribution in a closed interval is uniform.
- The maximum entropy distribution given its mean and variance is Gaussian.
- KL divergence is asymmetric.
- Entropy $H(p)$ is concave in $p$.
- Mutual information $I(x,y)$ is concave of $p(x)$ given $p(y|x)$ and convex of $p(y|x)$ given $p(x)$.